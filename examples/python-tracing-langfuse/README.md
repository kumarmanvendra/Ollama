# Trace your local Ollama model with Langfuse

## What is Langfuse?
Langfuse [(GitHub)](https://github.com/langfuse/langfuse) is an open-source LLM engineering platform. It includes features such as [traces](https://langfuse.com/docs/tracing), [evals](https://langfuse.com/docs/scores/overview), and [prompt management](https://langfuse.com/docs/prompts/get-started) to help you debug and improve your LLM app.

## Why use Tracing to gain Observability in an LLM Application?
- Capture the full context of execution, including API calls, context, prompts, parallelism, and more
- Monitor model usage and related costs
- Create datasets for fine-tuning and testing
- Effectively gather user feedback
- Detect and identify poor-quality outputs

## Langfuse and Ollama Integration Cookbook
In this cookbook, we will show you how to monitor and debug local language models with Ollama and Langfuse:

- [Langfuse Analytics for Ollama](langfuse_observability.ipynb)

## Feedback and Community
If you have any feedback or requests, please create a GitHub [Issue](https://langfuse.com/issue) or share your work with the community on [Discord](https://discord.langfuse.com/).